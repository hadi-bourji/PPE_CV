{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72dee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from torch.utils.data import DataLoader\n",
    "from yolox.test_weights import download_weights, load_pretrained_weights\n",
    "from yolox.model import create_yolox_s\n",
    "from data_utils.ppe_dataset import PPE_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515242ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from yolox/yolox_s.pth\n",
      "Loaded weights:\n",
      "  Missing keys: 0\n",
      "  Unexpected keys: 0\n"
     ]
    }
   ],
   "source": [
    "num_classes = 80\n",
    "weight_path = download_weights(\"yolox/yolox_s.pth\")\n",
    "model = create_yolox_s(num_classes)\n",
    "model = load_pretrained_weights(model, weight_path, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea48905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataset = PPE_DATA()\n",
    "\n",
    "def yolo_collate(batch):\n",
    "    imgs   = torch.stack([b[0] for b in batch])   #  (B,3,640,640)\n",
    "    labels = [b[1] for b in batch]                #  list of length B\n",
    "    return imgs, labels\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle = False, collate_fn=yolo_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9079ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9902, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9904, device='cuda:0')\n",
      "tensor(0.9820, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# img, label = dataset[4]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# img = img.unsqueeze(0)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ML/PPEDetection/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ML/PPEDetection/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ML/PPEDetection/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ML/PPEDetection/data_utils/ppe_dataset.py:55\u001b[39m, in \u001b[36mPPE_DATA.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     54\u001b[39m     img_path = \u001b[38;5;28mself\u001b[39m.file_names[idx]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     img = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# opencv uses bgr, so switch to standard rgb\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[32m     59\u001b[39m     lbl_path = img_path.replace(\u001b[33m'\u001b[39m\u001b[33m/images/\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m/labels/\u001b[39m\u001b[33m'\u001b[39m).rsplit(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for batch in loader:\n",
    "    img, label = batch\n",
    "    img = img.cuda()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        # noise = torch.randn_like(img).cuda()\n",
    "        output = model(img)\n",
    "        obj = output[..., 4:5]\n",
    "        print(obj.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a238e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5890, device='cuda:0')\n",
      "torch.Size([0]) torch.Size([0, 4]) torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "x1 = output[..., 0:1] - output[..., 2:3] / 2\n",
    "y1 = output[..., 1:2] - output[..., 3:4] / 2\n",
    "x2 = output[..., 0:1] + output[..., 2:3] / 2\n",
    "y2 = output[..., 1:2] + output[..., 3:4] / 2\n",
    "boxes = torch.cat([x1, y1, x2, y2], dim=-1)\n",
    "\n",
    "obj = output[..., 4:5]\n",
    "class_probs = output[..., 5:]\n",
    "scores = obj * class_probs\n",
    "print(torch.max(class_probs))\n",
    "best_scores, best_class = scores.max(dim=-1)\n",
    "\n",
    "mask = best_scores > 0.25\n",
    "best_scores, best_class, boxes = best_scores[mask], best_class[mask], boxes[mask]\n",
    "# TODO what abt batches??\n",
    "boxes = boxes.squeeze(0)\n",
    "best_scores = best_scores.squeeze(0)\n",
    "print(best_scores.shape, boxes.shape, best_class.shape)\n",
    "keep = nms(boxes, best_scores, iou_threshold=0.5)\n",
    "final_boxes = boxes[keep]\n",
    "final_classes = best_class.squeeze(0)[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b325188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2630, 5])\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(15., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n",
      "tensor(0., grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.cat((final_classes.unsqueeze(1), final_boxes), dim=1)\n",
    "print(labels.shape)\n",
    "if labels.shape[0] == 0:\n",
    "    print(\"No detections found.\")\n",
    "else:\n",
    "    PPE_DATA.show_img(img, labels[:20, :], rect_coords_centered=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
